{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código Antigo\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "# Intervalo de datas\n",
    "start_date = datetime(2025, 2, 1)\n",
    "end_date = datetime(2025, 4, 24)\n",
    "\n",
    "# Detalhes da API\n",
    "url_events = \"https://api.b365api.com/v3/events/ended\"\n",
    "token = \"183604-pWN7flhoAsWGu8\"\n",
    "\n",
    "def make_request_with_retry(url, params, retries=5, backoff_factor=2, timeout=10):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            logging.warning(f\"Tentativa {attempt} falhou: {e}. Retentando em {wait_time} segundos.\")\n",
    "            time.sleep(wait_time)\n",
    "            if attempt == retries:\n",
    "                logging.error(f\"Falha após {retries} tentativas para URL: {url}\")\n",
    "                raise e\n",
    "\n",
    "def fetch_events_for_date(date):\n",
    "    formatted_date = date.strftime('%Y%m%d')\n",
    "    params = {\n",
    "        \"token\": token,\n",
    "        \"sport_id\": \"1\",\n",
    "        \"league_id\": \"22614\",\n",
    "        \"day\": formatted_date,\n",
    "        \"page\": 1\n",
    "    }\n",
    "    events = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = make_request_with_retry(url_events, params)\n",
    "            data = response.json()\n",
    "            events.extend(data['results'])\n",
    "            if params['page'] * data['pager']['per_page'] < data['pager']['total']:\n",
    "                params['page'] += 1\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro ao buscar eventos para {formatted_date}: {e}\")\n",
    "            break\n",
    "    return events\n",
    "\n",
    "# Coletar eventos\n",
    "all_events = []\n",
    "for single_date in pd.date_range(start_date, end_date):\n",
    "    events = fetch_events_for_date(single_date)\n",
    "    all_events.extend(events)\n",
    "    logging.info(f\"Eventos coletados em {single_date.strftime('%Y-%m-%d')}: {len(events)}\")\n",
    "\n",
    "# Criar DataFrame\n",
    "df_events = pd.DataFrame([{\n",
    "    'event_id': event['id'],\n",
    "    'date': datetime.utcfromtimestamp(int(event['time'])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'away_team': event['away']['name'].split('(')[0].strip(),\n",
    "    'away_player': event['away']['name'].split('(')[1].split(')')[0] if '(' in event['away']['name'] else '',\n",
    "    'away_score': event.get('scores', {}).get('2', {}).get('away', 'N/A'),\n",
    "    'home_team': event['home']['name'].split('(')[0].strip(),\n",
    "    'home_player': event['home']['name'].split('(')[1].split(')')[0] if '(' in event['home']['name'] else '',\n",
    "    'home_score': event.get('scores', {}).get('2', {}).get('home', 'N/A'),\n",
    "} for event in all_events])\n",
    "\n",
    "# Processar colunas numéricas\n",
    "df_events['home_score'] = pd.to_numeric(df_events['gols_casa'].replace('N/A', np.nan), errors='coerce')\n",
    "df_events['away_score'] = pd.to_numeric(df_events['gols_fora'].replace('N/A', np.nan), errors='coerce')\n",
    "df_events['total_score'] = df_events['home_score'] + df_events['away_score']\n",
    "df_events.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Salvar dados brutos\n",
    "df_events.to_csv('22614_raw.csv', index=False)\n",
    "logging.info(\"Dados dos eventos salvos com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 17:51:36,307 - INFO - Collected 265 events for 2025-02-01\n",
      "2025-04-24 17:51:38,516 - INFO - Collected 218 events for 2025-02-02\n",
      "2025-04-24 17:51:48,135 - INFO - Collected 314 events for 2025-02-03\n",
      "2025-04-24 17:52:08,095 - INFO - Collected 374 events for 2025-02-04\n",
      "2025-04-24 17:52:13,492 - INFO - Collected 374 events for 2025-02-05\n",
      "2025-04-24 17:52:25,372 - INFO - Collected 380 events for 2025-02-06\n",
      "2025-04-24 17:52:29,695 - INFO - Collected 378 events for 2025-02-07\n",
      "2025-04-24 17:52:32,646 - INFO - Collected 235 events for 2025-02-08\n",
      "2025-04-24 17:52:36,447 - INFO - Collected 209 events for 2025-02-09\n",
      "2025-04-24 17:52:51,948 - INFO - Collected 326 events for 2025-02-10\n",
      "2025-04-24 17:53:03,635 - WARNING - Attempt 1 failed: HTTPSConnectionPool(host='api.b365api.com', port=443): Read timed out. (read timeout=10). Retrying in 2 seconds.\n",
      "2025-04-24 17:53:10,090 - INFO - Collected 380 events for 2025-02-11\n",
      "2025-04-24 17:53:13,061 - INFO - Collected 319 events for 2025-02-12\n",
      "2025-04-24 17:53:18,001 - INFO - Collected 343 events for 2025-02-13\n",
      "2025-04-24 17:53:25,618 - INFO - Collected 371 events for 2025-02-14\n",
      "2025-04-24 17:53:32,993 - INFO - Collected 272 events for 2025-02-15\n",
      "2025-04-24 17:53:35,428 - INFO - Collected 218 events for 2025-02-16\n",
      "2025-04-24 17:53:42,441 - INFO - Collected 318 events for 2025-02-17\n",
      "2025-04-24 17:53:53,065 - WARNING - Attempt 1 failed: HTTPSConnectionPool(host='api.b365api.com', port=443): Read timed out. (read timeout=10). Retrying in 2 seconds.\n",
      "2025-04-24 17:54:00,546 - INFO - Collected 374 events for 2025-02-18\n",
      "2025-04-24 17:54:12,407 - INFO - Collected 380 events for 2025-02-19\n",
      "2025-04-24 17:54:19,000 - INFO - Collected 372 events for 2025-02-20\n",
      "2025-04-24 17:54:25,031 - INFO - Collected 380 events for 2025-02-21\n",
      "2025-04-24 17:54:28,633 - INFO - Collected 274 events for 2025-02-22\n",
      "2025-04-24 17:54:30,855 - INFO - Collected 220 events for 2025-02-23\n",
      "2025-04-24 17:54:37,483 - INFO - Collected 326 events for 2025-02-24\n",
      "2025-04-24 17:54:58,036 - INFO - Collected 380 events for 2025-02-25\n",
      "2025-04-24 17:55:04,611 - INFO - Collected 374 events for 2025-02-26\n",
      "2025-04-24 17:55:08,053 - INFO - Collected 380 events for 2025-02-27\n",
      "2025-04-24 17:55:13,701 - INFO - Collected 380 events for 2025-02-28\n",
      "2025-04-24 17:55:16,556 - INFO - Collected 274 events for 2025-03-01\n",
      "2025-04-24 17:55:19,416 - INFO - Collected 220 events for 2025-03-02\n",
      "2025-04-24 17:55:27,144 - INFO - Collected 266 events for 2025-03-03\n",
      "2025-04-24 17:55:43,331 - INFO - Collected 378 events for 2025-03-04\n",
      "2025-04-24 17:55:51,278 - WARNING - Attempt 1 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8. Retrying in 2 seconds.\n",
      "2025-04-24 17:55:53,605 - WARNING - Attempt 2 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8. Retrying in 4 seconds.\n",
      "2025-04-24 17:55:57,941 - WARNING - Attempt 3 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8. Retrying in 8 seconds.\n",
      "2025-04-24 17:56:06,304 - WARNING - Attempt 4 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8. Retrying in 16 seconds.\n",
      "2025-04-24 17:56:22,620 - WARNING - Attempt 5 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8. Retrying in 32 seconds.\n",
      "2025-04-24 17:56:54,620 - ERROR - All 5 attempts failed for URL: https://api.b365api.com/v3/events/ended\n",
      "2025-04-24 17:56:54,621 - ERROR - Error fetching events for 20250305: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250305&page=8\n",
      "2025-04-24 17:56:54,622 - INFO - Collected 350 events for 2025-03-05\n",
      "2025-04-24 17:56:54,976 - WARNING - Attempt 1 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1. Retrying in 2 seconds.\n",
      "2025-04-24 17:56:57,287 - WARNING - Attempt 2 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1. Retrying in 4 seconds.\n",
      "2025-04-24 17:57:01,609 - WARNING - Attempt 3 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1. Retrying in 8 seconds.\n",
      "2025-04-24 17:57:09,935 - WARNING - Attempt 4 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1. Retrying in 16 seconds.\n",
      "2025-04-24 17:57:26,268 - WARNING - Attempt 5 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1. Retrying in 32 seconds.\n",
      "2025-04-24 17:57:58,271 - ERROR - All 5 attempts failed for URL: https://api.b365api.com/v3/events/ended\n",
      "2025-04-24 17:57:58,272 - ERROR - Error fetching events for 20250306: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250306&page=1\n",
      "2025-04-24 17:57:58,273 - INFO - Collected 0 events for 2025-03-06\n",
      "2025-04-24 17:57:58,594 - WARNING - Attempt 1 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250307&page=1. Retrying in 2 seconds.\n",
      "2025-04-24 17:58:03,469 - WARNING - Attempt 2 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250307&page=1. Retrying in 4 seconds.\n",
      "2025-04-24 17:58:07,804 - WARNING - Attempt 3 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250307&page=1. Retrying in 8 seconds.\n",
      "2025-04-24 17:58:16,116 - WARNING - Attempt 4 failed: 429 Client Error: Too Many Requests for url: https://api.b365api.com/v3/events/ended?token=183604-pWN7flhoAsWGu8&sport_id=1&league_id=22614&day=20250307&page=1. Retrying in 16 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Data Extractor\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_extractor.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Constants\n",
    "LEAGUE_ID = 22614\n",
    "START_DATE = datetime(2025, 2, 1)\n",
    "END_DATE = datetime(2025, 4, 24)\n",
    "API_TOKEN = \"183604-pWN7flhoAsWGu8\"\n",
    "EVENTS_URL = \"https://api.b365api.com/v3/events/ended\"\n",
    "ODDS_URL = \"https://api.b365api.com/v2/event/odds\"\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "MAX_CONSECUTIVE_ERRORS = 5\n",
    "\n",
    "# Column Definitions\n",
    "BASE_COLUMNS = [\n",
    "    'event_id', 'league_id', 'date', \n",
    "    'away_team', 'away_player', 'away_score',\n",
    "    'home_team', 'home_player', 'home_score',\n",
    "    'total_score'\n",
    "]\n",
    "\n",
    "ODDS_COLUMNS = [\n",
    "    '1_1_home_od', '1_1_draw_od', '1_1_away_od', '1_1_handicap',\n",
    "    '1_3_over_od', '1_3_under_od', '1_3_handicap',\n",
    "    'odds_timestamp'\n",
    "]\n",
    "\n",
    "def make_request_with_retry(url, params, retries=5, backoff_factor=2, timeout=10):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            logging.warning(f\"Attempt {attempt} failed: {e}. Retrying in {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "            if attempt == retries:\n",
    "                logging.error(f\"All {retries} attempts failed for URL: {url}\")\n",
    "                raise\n",
    "\n",
    "def fetch_events_for_date(date):\n",
    "    formatted_date = date.strftime('%Y%m%d')\n",
    "    params = {\n",
    "        \"token\": API_TOKEN,\n",
    "        \"sport_id\": \"1\",\n",
    "        \"league_id\": str(LEAGUE_ID),\n",
    "        \"day\": formatted_date,\n",
    "        \"page\": 1\n",
    "    }\n",
    "    \n",
    "    events = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = make_request_with_retry(EVENTS_URL, params)\n",
    "            data = response.json()\n",
    "            current_events = data.get('results', [])\n",
    "            \n",
    "            # Add league_id to each event\n",
    "            for event in current_events:\n",
    "                event['league_id'] = LEAGUE_ID\n",
    "                \n",
    "            events.extend(current_events)\n",
    "            \n",
    "            # Pagination handling\n",
    "            pager = data.get('pager', {})\n",
    "            if params['page'] * pager.get('per_page', 0) < pager.get('total', 0):\n",
    "                params['page'] += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching events for {formatted_date}: {e}\")\n",
    "            break\n",
    "            \n",
    "    return events\n",
    "\n",
    "def fetch_odds(event_id):\n",
    "    params = {'event_id': event_id, 'token': API_TOKEN}\n",
    "    try:\n",
    "        response = make_request_with_retry(ODDS_URL, params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get('success', 0) != 1:\n",
    "            return None\n",
    "            \n",
    "        return data.get('results', {}).get('odds', {})\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching odds for {event_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_odds(odds_data):\n",
    "    processed = {col: '-' for col in ODDS_COLUMNS}\n",
    "    \n",
    "    if not odds_data:\n",
    "        return processed\n",
    "        \n",
    "    try:\n",
    "        # Market 1_1 (Match Odds)\n",
    "        market_1_1 = odds_data.get('1_1', [])\n",
    "        if market_1_1:\n",
    "            latest = max(market_1_1, key=lambda x: int(x.get('add_time', 0)))\n",
    "            processed.update({\n",
    "                '1_1_home_od': latest.get('home_od', '-'),\n",
    "                '1_1_draw_od': latest.get('draw_od', '-'),\n",
    "                '1_1_away_od': latest.get('away_od', '-'),\n",
    "                '1_1_handicap': latest.get('handicap', '-')\n",
    "            })\n",
    "            \n",
    "        # Market 1_3 (Over/Under)\n",
    "        market_1_3 = odds_data.get('1_3', [])\n",
    "        if market_1_3:\n",
    "            valid_odds = [o for o in market_1_3 \n",
    "                        if o.get('ss') == '0-0' and \n",
    "                        o.get('over_od') not in ('-', '') and \n",
    "                        o.get('under_od') not in ('-', '')]\n",
    "            \n",
    "            if valid_odds:\n",
    "                best = min(valid_odds, key=lambda x: int(x.get('add_time', 0)))\n",
    "            else:\n",
    "                best = max(market_1_3, key=lambda x: int(x.get('add_time', 0)))\n",
    "                \n",
    "            processed.update({\n",
    "                '1_3_over_od': best.get('over_od', '-'),\n",
    "                '1_3_under_od': best.get('under_od', '-'),\n",
    "                '1_3_handicap': best.get('handicap', '-'),\n",
    "                'odds_timestamp': datetime.fromtimestamp(\n",
    "                    int(best.get('add_time', 0)), \n",
    "                    tz=timezone.utc\n",
    "                ).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing odds: {e}\")\n",
    "        \n",
    "    return processed\n",
    "\n",
    "def save_checkpoint(df, emergency=False):\n",
    "    filename = 'emergency_save.csv' if emergency else 'soccer_data.csv'\n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "        logging.info(f\"Checkpoint saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Collect Events\n",
    "    all_events = []\n",
    "    for date in pd.date_range(START_DATE, END_DATE):\n",
    "        try:\n",
    "            daily_events = fetch_events_for_date(date)\n",
    "            all_events.extend(daily_events)\n",
    "            logging.info(f\"Collected {len(daily_events)} events for {date.strftime('%Y-%m-%d')}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process {date}: {e}\")\n",
    "    \n",
    "    # Step 2: Create DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'event_id': event['id'],\n",
    "        'league_id': LEAGUE_ID,\n",
    "        'date': datetime.utcfromtimestamp(int(event['time'])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'away_team': event['away']['name'].split('(')[0].strip(),\n",
    "        'away_player': event['away']['name'].split('(')[1].split(')')[0] \n",
    "                      if '(' in event['away']['name'] else '',\n",
    "        'away_score': int(event.get('scores', {}).get('2', {}).get('away', 0)),\n",
    "        'home_team': event['home']['name'].split('(')[0].strip(),\n",
    "        'home_player': event['home']['name'].split('(')[1].split(')')[0] \n",
    "                      if '(' in event['home']['name'] else '',\n",
    "        'home_score': int(event.get('scores', {}).get('2', {}).get('home', 0)),\n",
    "    } for event in all_events])\n",
    "    \n",
    "    df['total_score'] = df['home_score'] + df['away_score']\n",
    "    \n",
    "    # Initialize odds columns\n",
    "    for col in ODDS_COLUMNS:\n",
    "        df[col] = '-'\n",
    "    \n",
    "    # Step 3: Fetch Odds with progress saving\n",
    "    consecutive_errors = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx > 0 and idx % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(df)\n",
    "            \n",
    "        try:\n",
    "            odds_data = fetch_odds(row['event_id'])\n",
    "            processed = process_odds(odds_data)\n",
    "            \n",
    "            for col in ODDS_COLUMNS:\n",
    "                df.at[idx, col] = processed.get(col, '-')\n",
    "                \n",
    "            consecutive_errors = 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process event {row['event_id']}: {e}\")\n",
    "            consecutive_errors += 1\n",
    "            \n",
    "            if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:\n",
    "                logging.critical(\"Maximum consecutive errors reached!\")\n",
    "                save_checkpoint(df, emergency=True)\n",
    "                exit()\n",
    "                \n",
    "    # Final Save\n",
    "    save_checkpoint(df)\n",
    "    logging.info(\"Processing completed successfully!\")\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
