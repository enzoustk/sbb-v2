{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:34:17,328 - INFO - Carregados 8445 eventos existentes\n",
      "2025-04-09 16:34:17,363 - INFO - Encontrados 18439 novos eventos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e18d6386d34be6876cea26b92f7e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando Odds:   0%|          | 0/18439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:34:18,052 - WARNING - Erro 429 detectado (1/3)\n",
      "2025-04-09 16:34:18,098 - WARNING - Erro 429 detectado (2/3)\n",
      "2025-04-09 16:34:18,360 - WARNING - Erro 429 detectado (3/3)\n",
      "2025-04-09 16:34:18,497 - ERROR - Limite de erros 429 atingido! Interrompendo...\n",
      "2025-04-09 16:34:21,115 - INFO - Nenhum novo dado foi processado\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Coletor de Odds - Versão Notebook\n",
    "# \n",
    "# **Funcionalidades:**\n",
    "# 1. Processa apenas eventos não coletados\n",
    "# 2. Para após 3 erros 429 consecutivos\n",
    "# 3. Salva progresso automaticamente\n",
    "# 4. Exibe progresso em tempo real\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Configurações Iniciais\n",
    "\n",
    "# %%\n",
    "# Configurações da API\n",
    "TOKEN = \"183604-pWN7flhoAsWGu8\"\n",
    "URL_ODDS = \"https://api.b365api.com/v2/event/odds\"\n",
    "\n",
    "# Controle de erros 429\n",
    "MAX_429_ERRORS = 3\n",
    "current_429_errors = 0\n",
    "stop_execution = False\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Funções Principais\n",
    "\n",
    "# %%\n",
    "def make_request(event_id):\n",
    "    global current_429_errors, stop_execution\n",
    "    \n",
    "    params = {\n",
    "        'token': TOKEN,\n",
    "        'event_id': event_id\n",
    "    }\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        if stop_execution:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(URL_ODDS, params=params, timeout=15)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                current_429_errors += 1\n",
    "                logging.warning(f\"Erro 429 detectado ({current_429_errors}/{MAX_429_ERRORS})\")\n",
    "                \n",
    "                if current_429_errors >= MAX_429_ERRORS:\n",
    "                    stop_execution = True\n",
    "                    logging.error(\"Limite de erros 429 atingido! Interrompendo...\")\n",
    "                    return None\n",
    "                \n",
    "                wait_time = (2 ** attempt) + 2\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Erro na tentativa {attempt+1}: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# %%\n",
    "def process_odds(event_id):\n",
    "    global stop_execution\n",
    "    \n",
    "    if stop_execution:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        data = make_request(event_id)\n",
    "        \n",
    "        if not data or data.get('success') != 1:\n",
    "            return None\n",
    "        \n",
    "        odds = data.get('results', {}).get('odds', {})\n",
    "        processed = {'event_id': event_id}\n",
    "        \n",
    "        # Processar mercados\n",
    "        for market in ['1_1', '1_2', '1_3']:\n",
    "            market_data = odds.get(market, [])\n",
    "            \n",
    "            # Selecionar odd mais relevante\n",
    "            valid_odds = [o for o in market_data if o.get('ss') == '0-0']\n",
    "            if not valid_odds:\n",
    "                valid_odds = [o for o in market_data if not o.get('ss')]\n",
    "            \n",
    "            if valid_odds:\n",
    "                best_odd = max(valid_odds, key=lambda x: int(x.get('add_time', 0)))\n",
    "                prefix = f\"{market}_\"\n",
    "                \n",
    "                if market == '1_3':\n",
    "                    processed.update({\n",
    "                        f\"{prefix}over\": best_odd.get('over_od'),\n",
    "                        f\"{prefix}under\": best_odd.get('under_od'),\n",
    "                        f\"{prefix}handicap\": best_odd.get('handicap')\n",
    "                    })\n",
    "                else:\n",
    "                    processed.update({\n",
    "                        f\"{prefix}home\": best_odd.get('home_od'),\n",
    "                        f\"{prefix}draw\": best_odd.get('draw_od'),\n",
    "                        f\"{prefix}away\": best_odd.get('away_od')\n",
    "                    })\n",
    "                \n",
    "                # Converter timestamp\n",
    "                ts = int(best_odd.get('add_time', 0))\n",
    "                processed[f\"{prefix}time\"] = datetime.fromtimestamp(ts, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro no evento {event_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Carregar Dados e Executar\n",
    "\n",
    "# %%\n",
    "# Carregar datasets\n",
    "try:\n",
    "    df_existing = pd.read_csv('dados_com_odds.csv')\n",
    "    existing_ids = set(df_existing['event_id'])\n",
    "    logging.info(f\"Carregados {len(df_existing)} eventos existentes\")\n",
    "except FileNotFoundError:\n",
    "    df_existing = pd.DataFrame()\n",
    "    existing_ids = set()\n",
    "    logging.info(\"Arquivo não encontrado - iniciando do zero\")\n",
    "\n",
    "df_events = pd.read_csv('dados_eventos.csv')\n",
    "new_events = df_events[~df_events['event_id'].isin(existing_ids)]\n",
    "\n",
    "if new_events.empty:\n",
    "    logging.info(\"Nenhum novo evento para processar\")\n",
    "else:\n",
    "    logging.info(f\"Encontrados {len(new_events)} novos eventos\")\n",
    "\n",
    "# %%\n",
    "# Executar coleta com progresso\n",
    "results = []\n",
    "batch_size = 10  # Reduza se encontrar erros 429\n",
    "\n",
    "with tqdm(total=len(new_events), desc=\"Processando Odds\") as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {executor.submit(process_odds, row.event_id): row for _, row in new_events.iterrows()}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            if stop_execution:\n",
    "                executor.shutdown(wait=False, cancel_futures=True)\n",
    "                break\n",
    "                \n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "            # Salvar progresso a cada batch\n",
    "            if len(results) % batch_size == 0:\n",
    "                df_temp = pd.DataFrame(results)\n",
    "                if not df_temp.empty:\n",
    "                    df_final = pd.concat([df_existing, df_temp], ignore_index=True)\n",
    "                    df_final.to_csv('dados_com_odds.csv', index=False)\n",
    "                    logging.info(f\"Progresso salvo - {len(df_final)} eventos\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Salvamento Final e Resultados\n",
    "\n",
    "# %%\n",
    "# Consolidar dados finais\n",
    "if results:\n",
    "    df_final = pd.concat([df_existing, pd.DataFrame(results)], ignore_index=True)\n",
    "    df_final.to_csv('dados_com_odds.csv', index=False)\n",
    "    df_final.to_excel('dados_com_odds.xlsx', index=False)\n",
    "    logging.info(f\"Processo completo! Total de eventos: {len(df_final)}\")\n",
    "    \n",
    "    # Exibir amostra\n",
    "    display(df_final.tail(3))\n",
    "else:\n",
    "    logging.info(\"Nenhum novo dado foi processado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
