{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrator de Dados\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuração de Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Definição do intervalo de datas para coleta de dados\n",
    "start_date = datetime(2025, 1, 17)\n",
    "end_date = datetime(2025, 1, 29)\n",
    "\n",
    "# Detalhes da requisição da API\n",
    "url_events = \"https://api.b365api.com/v3/events/ended\"\n",
    "url_odds = \"https://api.b365api.com/v2/event/odds\"\n",
    "token = \"183604-pWN7flhoAsWGu8\"  # Substitua pelo seu token real\n",
    "\n",
    "# Função genérica para requisições com retries\n",
    "def make_request_with_retry(url, params, retries=5, backoff_factor=2, timeout=10):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=timeout)\n",
    "            response.raise_for_status()  # Levanta exceção se status_code >= 400\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt  # Exponential backoff\n",
    "            logging.warning(f\"Tentativa {attempt} falhou: {e}. Retentando em {wait_time} segundos.\")\n",
    "            time.sleep(wait_time)\n",
    "            if attempt == retries:\n",
    "                logging.error(f\"Todas as {retries} tentativas falharam para URL: {url}\")\n",
    "                raise e\n",
    "\n",
    "# Função para buscar eventos em uma data específica\n",
    "def fetch_events_for_date(date):\n",
    "    formatted_date = date.strftime('%Y%m%d')\n",
    "    params = {\n",
    "        \"token\": token,\n",
    "        \"sport_id\": \"1\",\n",
    "        \"league_id\": \"22614\",\n",
    "        \"day\": formatted_date,\n",
    "        \"page\": 1\n",
    "    }\n",
    "    events = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = make_request_with_retry(url_events, params)\n",
    "            data = response.json()\n",
    "            events.extend(data['results'])\n",
    "            if params['page'] * data['pager']['per_page'] < data['pager']['total']:\n",
    "                params['page'] += 1\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro ao buscar eventos para a data {formatted_date}: {e}\")\n",
    "            break\n",
    "    return events\n",
    "\n",
    "# Função para pegar os dados da API de odds\n",
    "def fetch_odds_for_event(event_id):\n",
    "    params = {\n",
    "        'event_id': event_id,\n",
    "        'token': token\n",
    "    }\n",
    "    try:\n",
    "        response = make_request_with_retry(url_odds, params)\n",
    "        data = response.json()\n",
    "        if 'success' in data and data['success'] == 1:\n",
    "            results = data.get('results', {})\n",
    "            odds = results.get('odds', {})\n",
    "\n",
    "            markets = ['1_1', '1_2', '1_3']\n",
    "            filtered_odds = {}\n",
    "\n",
    "            for market in markets:\n",
    "                market_odds = odds.get(market, [])\n",
    "                selected_odd = None\n",
    "                min_add_time = float('inf')\n",
    "\n",
    "                for odd in market_odds:\n",
    "                    add_time = int(odd.get('add_time'))\n",
    "                    ss = odd.get('ss')\n",
    "\n",
    "                    if ss == '0-0' and add_time < min_add_time:\n",
    "                        min_add_time = add_time\n",
    "                        selected_odd = odd\n",
    "\n",
    "                if not selected_odd:\n",
    "                    max_add_time = float('-inf')\n",
    "                    for odd in market_odds:\n",
    "                        add_time = int(odd.get('add_time'))\n",
    "                        ss = odd.get('ss')\n",
    "\n",
    "                        if ss is None and add_time > max_add_time:\n",
    "                            max_add_time = add_time\n",
    "                            selected_odd = odd\n",
    "\n",
    "                if selected_odd:\n",
    "                    if market == '1_3':\n",
    "                        filtered_odds[market] = {\n",
    "                            'over_od': selected_odd.get('over_od'),\n",
    "                            'under_od': selected_odd.get('under_od'),\n",
    "                            'handicap': selected_odd.get('handicap'),\n",
    "                            'add_time': datetime.fromtimestamp(int(selected_odd.get('add_time')), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        }\n",
    "                    else:\n",
    "                        filtered_odds[market] = {\n",
    "                            'home_od': selected_odd.get('home_od'),\n",
    "                            'draw_od': selected_odd.get('draw_od'),\n",
    "                            'away_od': selected_odd.get('away_od'),\n",
    "                            'handicap': selected_odd.get('handicap'),\n",
    "                            'add_time': datetime.fromtimestamp(int(selected_odd.get('add_time')), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        }\n",
    "                else:\n",
    "                    filtered_odds[market] = None\n",
    "            return filtered_odds\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao buscar odds para o evento {event_id}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Coleta de eventos para cada data no intervalo\n",
    "all_events = []\n",
    "for single_date in pd.date_range(start_date, end_date):\n",
    "    events = fetch_events_for_date(single_date)\n",
    "    all_events.extend(events)\n",
    "    logging.info(f\"Fetched {len(events)} events for {single_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Criando o DataFrame com os eventos coletados\n",
    "df_events = pd.DataFrame([{\n",
    "    'event_id': event['id'],\n",
    "    'date': datetime.utcfromtimestamp(int(event['time'])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'time_fora': event['away']['name'].split('(')[0].strip(),\n",
    "    'jogador_fora': event['away']['name'].split('(')[1].split(')')[0] if '(' in event['away']['name'] else '',\n",
    "    'gols_fora': event.get('scores', {}).get('2', {}).get('away', 'N/A'),\n",
    "    'time_casa': event['home']['name'].split('(')[0].strip(),\n",
    "    'jogador_casa': event['home']['name'].split('(')[1].split(')')[0] if '(' in event['home']['name'] else '',\n",
    "    'gols_casa': event.get('scores', {}).get('2', {}).get('home', 'N/A'),\n",
    "    #'corners_fora': event.get('stats', {}).get('corners', [np.nan, np.nan])[1],\n",
    "    #'corners_casa': event.get('stats', {}).get('corners', [np.nan, np.nan])[0],\n",
    "} for event in all_events])\n",
    "\n",
    "# Conversão de 'gols_casa', 'gols_fora', 'corners_casa' e 'corners_fora' para numérico e remoção de linhas com NaN\n",
    "df_events['gols_casa'] = pd.to_numeric(df_events['gols_casa'].replace('N/A', np.nan), errors='coerce')\n",
    "df_events['gols_fora'] = pd.to_numeric(df_events['gols_fora'].replace('N/A', np.nan), errors='coerce')\n",
    "#df_events['corners_casa'] = pd.to_numeric(df_events['corners_casa'].replace('N/A', np.nan), errors='coerce')\n",
    "#df_events['corners_fora'] = pd.to_numeric(df_events['corners_fora'].replace('N/A', np.nan), errors='coerce')\n",
    "\n",
    "# Adicionando a coluna 'gols_totais' pela soma de 'gols_casa' e 'gols_fora'\n",
    "df_events['gols_totais'] = df_events['gols_casa'] + df_events['gols_fora']\n",
    "\n",
    "# Ordenação dos eventos por data\n",
    "df_events.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Função para processar cada linha para odds\n",
    "def process_event_odds(index, row):\n",
    "    event_id = row['event_id']\n",
    "    logging.info(f\"Fetching odds for event_id: {event_id}\")\n",
    "    try:\n",
    "        odds_data = fetch_odds_for_event(event_id)\n",
    "        resultado = {}\n",
    "\n",
    "        if odds_data:\n",
    "            for market, odds in odds_data.items():\n",
    "                if odds:\n",
    "                    if market == '1_3':\n",
    "                        resultado[f'{market}_over_od'] = odds['over_od']\n",
    "                        resultado[f'{market}_under_od'] = odds['under_od']\n",
    "                        resultado[f'{market}_handicap'] = odds['handicap']\n",
    "                    #else:\n",
    "                        #resultado[f'{market}_home_od'] = odds['home_od']\n",
    "                        #resultado[f'{market}_draw_od'] = odds['draw_od']\n",
    "                        #resultado[f'{market}_away_od'] = odds['away_od']\n",
    "                        #resultado[f'{market}_handicap'] = odds['handicap']\n",
    "                        \n",
    "                    #resultado[f'{market}_add_time'] = odds['add_time']\n",
    "\n",
    "        return index, resultado\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao buscar odds para o evento {event_id}: {e}\")\n",
    "        return index, {}\n",
    "\n",
    "# Adicionando odds ao DataFrame de eventos\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(process_event_odds, index, row) for index, row in df_events.iterrows()]\n",
    "    for future in as_completed(futures):\n",
    "        index, odds_result = future.result()\n",
    "        for col, value in odds_result.items():\n",
    "            df_events.loc[index, col] = value\n",
    "        logging.info(f\"Odds added for event_id at index {index}\")\n",
    "\n",
    "# Exibição dos primeiros registros para verificação\n",
    "display(df_events.head())\n",
    "\n",
    "# Salvar o DataFrame atualizado com odds\n",
    "df_events.to_csv('dados_8min.csv', index=False)\n",
    "df_events.to_excel('dados_8min.xlsx', index=False)\n",
    "logging.info(\"Processamento concluído e dados salvos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 10:42:14,375 - INFO - Coletados 378 eventos para liga 22614 em 2025-01-17\n",
      "2025-02-05 10:42:19,297 - INFO - Coletados 256 eventos para liga 23114 em 2025-01-17\n",
      "2025-02-05 10:42:24,766 - INFO - Coletados 161 eventos para liga 37298 em 2025-01-17\n",
      "2025-02-05 10:42:34,025 - INFO - Coletados 280 eventos para liga 38439 em 2025-01-17\n",
      "2025-02-05 10:42:38,936 - INFO - Coletados 268 eventos para liga 22614 em 2025-01-18\n",
      "2025-02-05 10:42:46,409 - INFO - Coletados 304 eventos para liga 23114 em 2025-01-18\n",
      "2025-02-05 10:42:49,592 - INFO - Coletados 183 eventos para liga 37298 em 2025-01-18\n",
      "2025-02-05 10:42:54,670 - INFO - Coletados 200 eventos para liga 38439 em 2025-01-18\n",
      "2025-02-05 10:42:58,963 - INFO - Coletados 220 eventos para liga 22614 em 2025-01-19\n",
      "2025-02-05 10:43:05,828 - INFO - Coletados 276 eventos para liga 23114 em 2025-01-19\n",
      "2025-02-05 10:43:08,081 - INFO - Coletados 149 eventos para liga 37298 em 2025-01-19\n",
      "2025-02-05 10:43:13,124 - INFO - Coletados 200 eventos para liga 38439 em 2025-01-19\n",
      "2025-02-05 10:43:21,660 - INFO - Coletados 326 eventos para liga 22614 em 2025-01-20\n",
      "2025-02-05 10:43:28,598 - INFO - Coletados 277 eventos para liga 23114 em 2025-01-20\n",
      "2025-02-05 10:43:31,840 - INFO - Coletados 165 eventos para liga 37298 em 2025-01-20\n",
      "2025-02-05 10:43:37,152 - INFO - Coletados 274 eventos para liga 38439 em 2025-01-20\n",
      "2025-02-05 10:43:43,885 - INFO - Coletados 380 eventos para liga 22614 em 2025-01-21\n",
      "2025-02-05 10:43:48,711 - INFO - Coletados 277 eventos para liga 23114 em 2025-01-21\n",
      "2025-02-05 10:43:50,912 - INFO - Coletados 144 eventos para liga 37298 em 2025-01-21\n"
     ]
    }
   ],
   "source": [
    "# Extrator de Dados\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuração de Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Definição do intervalo de datas para coleta de dados\n",
    "start_date = datetime(2025, 1, 17)\n",
    "end_date = datetime(2025, 2, 4)\n",
    "\n",
    "# Detalhes da requisição da API\n",
    "url_events = \"https://api.b365api.com/v3/events/ended\"\n",
    "url_odds = \"https://api.b365api.com/v2/event/odds\"\n",
    "token = \"183604-pWN7flhoAsWGu8\"  # Substitua pelo seu token real\n",
    "league_ids = [22614, 23114, 37298, 38439]  # Lista de ligas a serem coletadas\n",
    "\n",
    "# Função genérica para requisições com retries\n",
    "def make_request_with_retry(url, params, retries=5, backoff_factor=2, timeout=10):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=timeout)\n",
    "            response.raise_for_status()  # Levanta exceção se status_code >= 400\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt  # Exponential backoff\n",
    "            logging.warning(f\"Tentativa {attempt} falhou: {e}. Retentando em {wait_time} segundos.\")\n",
    "            time.sleep(wait_time)\n",
    "            if attempt == retries:\n",
    "                logging.error(f\"Todas as {retries} tentativas falharam para URL: {url}\")\n",
    "                raise e\n",
    "\n",
    "# Função para buscar eventos em uma data e liga específicas\n",
    "def fetch_events_for_date(date, league_id):\n",
    "    formatted_date = date.strftime('%Y%m%d')\n",
    "    params = {\n",
    "        \"token\": token,\n",
    "        \"sport_id\": \"1\",\n",
    "        \"league_id\": str(league_id),\n",
    "        \"day\": formatted_date,\n",
    "        \"page\": 1\n",
    "    }\n",
    "    events = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = make_request_with_retry(url_events, params)\n",
    "            data = response.json()\n",
    "            current_events = data['results']\n",
    "            # Adiciona league_id a cada evento\n",
    "            for event in current_events:\n",
    "                event['league_id'] = league_id\n",
    "            events.extend(current_events)\n",
    "            if params['page'] * data['pager']['per_page'] < data['pager']['total']:\n",
    "                params['page'] += 1\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro ao buscar eventos para a data {formatted_date} e liga {league_id}: {e}\")\n",
    "            break\n",
    "    return events\n",
    "\n",
    "# Função para pegar os dados da API de odds\n",
    "def fetch_odds_for_event(event_id):\n",
    "    params = {\n",
    "        'event_id': event_id,\n",
    "        'token': token\n",
    "    }\n",
    "    try:\n",
    "        response = make_request_with_retry(url_odds, params)\n",
    "        data = response.json()\n",
    "        if 'success' in data and data['success'] == 1:\n",
    "            results = data.get('results', {})\n",
    "            odds = results.get('odds', {})\n",
    "\n",
    "            markets = ['1_1', '1_2', '1_3']\n",
    "            filtered_odds = {}\n",
    "\n",
    "            for market in markets:\n",
    "                market_odds = odds.get(market, [])\n",
    "                selected_odd = None\n",
    "                min_add_time = float('inf')\n",
    "\n",
    "                for odd in market_odds:\n",
    "                    add_time = int(odd.get('add_time'))\n",
    "                    ss = odd.get('ss')\n",
    "\n",
    "                    if ss == '0-0' and add_time < min_add_time:\n",
    "                        min_add_time = add_time\n",
    "                        selected_odd = odd\n",
    "\n",
    "                if not selected_odd:\n",
    "                    max_add_time = float('-inf')\n",
    "                    for odd in market_odds:\n",
    "                        add_time = int(odd.get('add_time'))\n",
    "                        ss = odd.get('ss')\n",
    "\n",
    "                        if ss is None and add_time > max_add_time:\n",
    "                            max_add_time = add_time\n",
    "                            selected_odd = odd\n",
    "\n",
    "                if selected_odd:\n",
    "                    if market == '1_3':\n",
    "                        filtered_odds[market] = {\n",
    "                            'over_od': selected_odd.get('over_od'),\n",
    "                            'under_od': selected_odd.get('under_od'),\n",
    "                            'handicap': selected_odd.get('handicap'),\n",
    "                            'add_time': datetime.fromtimestamp(int(selected_odd.get('add_time')), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        }\n",
    "                    else:\n",
    "                        filtered_odds[market] = {\n",
    "                            'home_od': selected_odd.get('home_od'),\n",
    "                            'draw_od': selected_odd.get('draw_od'),\n",
    "                            'away_od': selected_odd.get('away_od'),\n",
    "                            'handicap': selected_odd.get('handicap'),\n",
    "                            'add_time': datetime.fromtimestamp(int(selected_odd.get('add_time')), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        }\n",
    "                else:\n",
    "                    filtered_odds[market] = None\n",
    "            return filtered_odds\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao buscar odds para o evento {event_id}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Coleta de eventos para cada data e liga no intervalo\n",
    "all_events = []\n",
    "for single_date in pd.date_range(start_date, end_date):\n",
    "    for league_id in league_ids:\n",
    "        events = fetch_events_for_date(single_date, league_id)\n",
    "        all_events.extend(events)\n",
    "        logging.info(f\"Coletados {len(events)} eventos para liga {league_id} em {single_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Criando o DataFrame com os eventos coletados\n",
    "df_events = pd.DataFrame([{\n",
    "    'event_id': event['id'],\n",
    "    'league': event['league_id'],  # Nova coluna adicionada\n",
    "    'date': datetime.utcfromtimestamp(int(event['time'])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'time_fora': event['away']['name'].split('(')[0].strip(),\n",
    "    'jogador_fora': event['away']['name'].split('(')[1].split(')')[0] if '(' in event['away']['name'] else '',\n",
    "    'gols_fora': int(event.get('scores', {}).get('2', {}).get('away', 0)),  # Convertendo para inteiro\n",
    "    'time_casa': event['home']['name'].split('(')[0].strip(),\n",
    "    'jogador_casa': event['home']['name'].split('(')[1].split(')')[0] if '(' in event['home']['name'] else '',\n",
    "    'gols_casa': int(event.get('scores', {}).get('2', {}).get('home', 0)),  # Convertendo para inteiro\n",
    "} for event in all_events])\n",
    "\n",
    "# Conversão de 'gols_casa', 'gols_fora', 'corners_casa' e 'corners_fora' para numérico e remoção de linhas com NaN\n",
    "df_events['gols_casa'] = pd.to_numeric(df_events['gols_casa'].replace('N/A', np.nan), errors='coerce')\n",
    "df_events['gols_fora'] = pd.to_numeric(df_events['gols_fora'].replace('N/A', np.nan), errors='coerce')\n",
    "\n",
    "df_events['gols_totais'] = df_events['gols_casa'] + df_events['gols_fora']\n",
    "\n",
    "# Função para processar cada linha para odds\n",
    "def process_event_odds(index, row):\n",
    "    event_id = row['event_id']\n",
    "    logging.info(f\"Buscando odds para event_id: {event_id}\")\n",
    "    try:\n",
    "        odds_data = fetch_odds_for_event(event_id)\n",
    "        resultado = {}\n",
    "\n",
    "        if odds_data:\n",
    "            for market, odds in odds_data.items():\n",
    "                if odds:\n",
    "                    if market == '1_3':\n",
    "                        resultado[f'{market}_over_od'] = odds['over_od']\n",
    "                        resultado[f'{market}_under_od'] = odds['under_od']\n",
    "                        resultado[f'{market}_handicap'] = odds['handicap']\n",
    "                    else:\n",
    "                        resultado[f'{market}_home_od'] = odds['home_od']\n",
    "                        resultado[f'{market}_draw_od'] = odds['draw_od']\n",
    "                        resultado[f'{market}_away_od'] = odds['away_od']\n",
    "                        resultado[f'{market}_handicap'] = odds['handicap']\n",
    "                        \n",
    "                    resultado[f'{market}_add_time'] = odds['add_time']\n",
    "\n",
    "        return index, resultado\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao buscar odds para o evento {event_id}: {e}\")\n",
    "        return index, {}\n",
    "\n",
    "# Adicionando odds ao DataFrame de eventos\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(process_event_odds, index, row) for index, row in df_events.iterrows()]\n",
    "    for future in as_completed(futures):\n",
    "        index, odds_result = future.result()\n",
    "        for col, value in odds_result.items():\n",
    "            df_events.loc[index, col] = value\n",
    "        logging.info(f\"Odds adicionadas para event_id no índice {index}\")\n",
    "\n",
    "# Exibição dos primeiros registros para verificação\n",
    "display(df_events.head())\n",
    "\n",
    "# Salvamento dos dados\n",
    "df_events.to_csv('dados_totais.csv', index=False)\n",
    "df_events.to_excel('dados_totais.xlsx', index=False)\n",
    "logging.info(\"Processamento concluído e dados salvos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
